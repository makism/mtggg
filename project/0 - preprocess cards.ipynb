{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the set from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'dataset/THB.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, 'r') as fp:\n",
    "    raw_data = json.load(fp)\n",
    "    cards = raw_data['cards']\n",
    "\n",
    "n = len(cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the cards in a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/THB_cards.json', 'w') as fp:\n",
    "    json.dump(cards, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    \"When {CARDNAME} enters the battlefield\": \"ETB_EFFECT\",\n",
    "    \n",
    "    \"Flash (You may cast this spell any time you could cast an instant.)\": \"FLASH\",\n",
    "    \"Flash\": \"FLASH\",\n",
    "    \n",
    "    \"Reach (This creature can block creatures with flying.)\": \"REACH\",\n",
    "    \"Reach\": \"REACH\",\n",
    "    \n",
    "    \"Flying (This creature can't be blocked except by creatures with flying or reach.)\": \"FLYING\",\n",
    "    \"Flying\": \"FLYING\",\n",
    "    \n",
    "    \"Haste (This creature can attack and {T} as soon as it comes under your control.)\": \"HASTE\",\n",
    "    \"Haste\": \"HASTE\",\n",
    "    \n",
    "    \"Trample (This creature can deal excess combat damage to the player or planeswalker it's attacking.)\": \"TRAMPLE\",\n",
    "    \"Trample\": \"TRAMPLE\",\n",
    "    \n",
    "    \"Vigilance (Attacking doesn't cause this creature to tap.)\": \"VIGILANCE\",\n",
    "    \"Vigilance\": \" VIGILANCE\",\n",
    "\n",
    "    \"Double strike (This creature deals both first-strike and regular combat damage.)\": \"DOUBLE_STRIKE\",\n",
    "    \"Double strike\": \"DOUBLE_STRIKE\",\n",
    "\n",
    "    \"Deathtouch (Any amount of damage this deals to a creature is enough to destroy it.)\": \"DEATHTOUCH\",\n",
    "    \"Deathtouch\": \"DEATHTOUCH\",\n",
    "    \n",
    "    \"Protection from green (This creature can't be blocked, targeted, dealt damage, enchanted, or equipped by anything green.)\": \"PROTECTION_FROM_GREEN\",\n",
    "    \"Protection from red (This creature can't be blocked, targeted, dealt damage, enchanted, or equipped by anything red.)\": \"PROTECTION_FROM_RED\",\n",
    "    \"Protection from black (This creature can't be blocked, targeted, dealt damage, enchanted, or equipped by anything black.)\": \"PROTECTION_FROM_BLACK\",\n",
    "    \"Protection from blue (This creature can't be blocked, targeted, dealt damage, enchanted, or equipped by anything blue.)\": \"PROTECTION_FROM_BLUE\",\n",
    "    \"Protection from white (This creature can't be blocked, targeted, dealt damage, enchanted, or equipped by anything white.)\": \"PROTECTION_FROM_WHITE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def udf_filter_text(name, text):\n",
    "    if isinstance(text, str):\n",
    "        new_text = text\n",
    "        new_text = new_text.replace(name, 'CARDNAME')\n",
    "        for line in new_text:\n",
    "            for rule, replace in rules.items():\n",
    "                new_text = new_text.replace(rule, replace)\n",
    "\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('dataset/THB_cards.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out duplicate cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_names = df.select(['number', 'name']).toPandas()\n",
    "unique_names, indices, counts = np.unique(pd_names['name'], return_index=True, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_unique_names = pd_names.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = spark.createDataFrame(pd_unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filter.join(df, on='number', how='left').drop(df_filter.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cards  = df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of cards 273\n"
     ]
    }
   ],
   "source": [
    "print(f'Final number of cards {num_cards}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.withColumn('filtered_text', udf_filter_text('name', 'text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode the color identity in a string, separated by \",\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colorIdentities = df_filtered.selectExpr([\"number\", \"colorIdentity\"]).select('number', fn.expr(\"concat_ws(',', colorIdentity)\").alias(\"str_colorIdentity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.join(df_colorIdentities, on='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the color identity to discrete ints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn + pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sklearn\n",
    "# from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colorId = df_filtered.select(\"str_colorIdentity\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_allColorIds = df_filtered.select(\"str_colorIdentity\").toPandas()\n",
    "# pd_colorIds = df_colorId.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = preprocessing.LabelEncoder().fit(pd_colorIds)\n",
    "# encoded_colorIds = le.transform(pd_allColorIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_colorIds = pd.DataFrame(encoded_colorIds, columns=['encoded_colorIdentity'])\n",
    "# pd_colorIds['number'] = df_filtered.select(\"number\").toPandas().astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded_colorIds = spark.createDataFrame(pd_colorIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df_filtered.join(df_encoded_colorIds, on='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark + ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"str_colorIdentity\", outputCol=\"encoded_colorIdentity\", stringOrderType='alphabetAsc')\n",
    "df_filtered = indexer.fit(df_filtered).transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|str_colorIdentity|encoded_colorIdentity|\n",
      "+-----------------+---------------------+\n",
      "|                G|                  6.0|\n",
      "|                R|                 10.0|\n",
      "|                R|                 10.0|\n",
      "|                U|                 13.0|\n",
      "|                W|                 15.0|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.select([\"str_colorIdentity\", \"encoded_colorIdentity\"]).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|str_colorIdentity|encoded_colorIdentity|\n",
      "+-----------------+---------------------+\n",
      "|              B,U|                  4.0|\n",
      "|                 |                  0.0|\n",
      "|              G,W|                  9.0|\n",
      "|              G,R|                  7.0|\n",
      "|                U|                 13.0|\n",
      "|                W|                 15.0|\n",
      "|              U,W|                 14.0|\n",
      "|                G|                  6.0|\n",
      "|              G,U|                  8.0|\n",
      "|              B,W|                  5.0|\n",
      "|              B,G|                  2.0|\n",
      "|              R,W|                 12.0|\n",
      "|                B|                  1.0|\n",
      "|              B,R|                  3.0|\n",
      "|              R,U|                 11.0|\n",
      "|                R|                 10.0|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.select([\"str_colorIdentity\", \"encoded_colorIdentity\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create an SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.createOrReplaceTempView(\"cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CAST(number as Integer), \n",
    "        rarity,\n",
    "        name,\n",
    "        CAST(convertedManaCost as Integer),\n",
    "        CAST(encoded_colorIdentity as Integer),\n",
    "        str_colorIdentity as colorIdentity,\n",
    "        text as originalText,\n",
    "        filtered_text as filteredText,\n",
    "        CAST(power as Integer),\n",
    "        CAST(toughness as Integer)\n",
    "    FROM\n",
    "        cards\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.write.mode(\"overwrite\").parquet('/tmp/THB_cards.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this fails\n",
    "# tbl.write.mode(\"overwrite\").parquet('dataset/THB_cards.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf \"dataset/THB_cards.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv \"/tmp/THB_cards.parquet\" \"dataset/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
